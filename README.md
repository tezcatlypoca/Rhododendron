# ğŸ¤– Crew AI Locale

Un systÃ¨me d'agents AI collaboratifs exÃ©cutÃ©s entiÃ¨rement en local pour le dÃ©veloppement de petits projets logiciels.

ğŸ“‚ [Voir la structure du projet](docs/Architecture%20dossier%20du%20projet.md)

## ğŸ” Vue d'ensemble

Ce projet implÃ©mente une Ã©quipe d'agents d'intelligence artificielle (manager, dÃ©veloppeur, testeur) qui collaborent pour concevoir, crÃ©er et tester des projets logiciels. Contrairement aux solutions basÃ©es sur le cloud, tous les composants s'exÃ©cutent localement sur votre machine.

### âœ¨ CaractÃ©ristiques principales

- ğŸ  **100% Local** : Aucune dÃ©pendance aux API externes
- ğŸ§  **Agents spÃ©cialisÃ©s** : Manager, dÃ©veloppeur et testeur travaillant ensemble
- ğŸ“Š **Dashboard** : Interface utilisateur pour suivre et gÃ©rer les agents
- ğŸ› ï¸ **Adaptable** : Fonctionne avec des contraintes matÃ©rielles raisonnables

## ğŸ—ï¸ Architecture

Notre systÃ¨me est structurÃ© en couches:

| Couche | Composants | RÃ´le |
|--------|------------|------|
| **UI** | Streamlit, FastAPI | Interface utilisateur et API |
| **Agents** | Manager, Developer, Tester | RÃ´les spÃ©cialisÃ©s pour diffÃ©rentes tÃ¢ches |
| **Orchestration** | LangChain, CrewAI | Gestion des workflows et interactions |
| **ModÃ¨les** | Llama 3, CodeLlama, Mistral | ModÃ¨les de fondation pour les agents |
| **Hardware** | GPU, CPU, RAM | Ressources matÃ©rielles |

## ğŸ’» PrÃ©requis matÃ©riels

### Configuration minimale
- CPU: 6 cÅ“urs / 12 threads
- RAM: 16 GB (32 GB recommandÃ©)
- GPU: 8 GB VRAM (NVIDIA prÃ©fÃ©rÃ©)
- Stockage: SSD 1 TB

### Configuration idÃ©ale
- CPU: 8+ cÅ“urs
- RAM: 32-64 GB
- GPU: NVIDIA RTX 3060+ (12GB+ VRAM)
- Stockage: SSD NVMe 2 TB

## ğŸš€ Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/tezcatlypoca/Rhododendron.git
cd Rhododendron

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sous Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer le dashboard
streamlit run app.py
```

### Installation du model Ollama

- Se rendre sur [Ollama](https://ollama.ai/)
- TÃ©lÃ©charger Ollama, puis run la commande suivante:
  
```bash
ollama pull codellama:7b-instruct-q4_0
```


## ğŸ“¦ Technologies

- **ModÃ¨les**: [Llama 3](https://ai.meta.com/llama/), [CodeLlama](https://github.com/facebookresearch/codellama), [Mistral](https://mistral.ai/)
- **Orchestration**: [LangChain](https://www.langchain.com/), [CrewAI](https://www.crewai.io/)
- **ExÃ©cution**: [ollama](https://ollama.ai/), [vLLM](https://github.com/vllm-project/vllm)
- **Interface**: [Streamlit](https://streamlit.io/), [FastAPI](https://fastapi.tiangolo.com/)

## ğŸ“ Feuille de route

- [x] Conception de l'architecture
- [ ] Configuration de l'environnement de base
- [ ] ImplÃ©mentation de l'agent Manager
- [ ] ImplÃ©mentation de l'agent Developer
- [ ] ImplÃ©mentation de l'agent Tester
- [ ] Orchestration inter-agents
- [ ] DÃ©veloppement du dashboard
- [ ] Tests et optimisations

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Consultez [CONTRIBUTING.md](CONTRIBUTING.md) pour les dÃ©tails.

## ğŸ“„ Licence

Ce projet est sous licence [MIT](LICENSE).

## ğŸ“š Ressources

- [Documentation LangChain](https://python.langchain.com/en/latest/)
- [Guide de quantification des modÃ¨les](https://huggingface.co/docs/transformers/main/en/quantization)
- [Tutoriel CrewAI](https://www.crewai.io/docs/tutorials/getting-started)
